{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad401fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "ignores = ['datetime', 'postgres_load_data', 'postgres_gucs_shared_memory_size', \n",
    "          'postgres_gucs_shared_memory_size_in_huge_pages', \n",
    "           'postgres_build_sha', 'postgres_build_compile_options']\n",
    "\n",
    "ignores2 = [\n",
    "    'machine_disk_block_device_settings_nr_hw_queues', \n",
    "#     'machine_disk_block_device_settings_read_ahead_kb',\n",
    "    'machine_disk_block_device_settings_nr_requests', \n",
    "    'machine_disk_block_device_settings_queue_depth',\n",
    "    'machine_disk_block_device_settings_max_sectors_kb', 'machine_disk_block_device_settings_scheduler', \n",
    "    'machine_disk_block_device_settings_wbt_lat_usec', 'machine_disk_block_device_settings_rotational',\n",
    "    'machine_disk_filesystem_source', \n",
    "    'postgres_gucs_track_io_timing', 'machine_instance_huge_pages_size_kb',\n",
    "    'machine_instance_mem_total_bytes', 'machine_instance_hostinfo_KernelVersion',\n",
    "    'machine_instance_hostinfo_KernelRelease', 'machine_disk_io_latency',\n",
    "    'benchmark_config_custom_filename', \n",
    "    'machine_disk_io_latency',\n",
    "    'postgres_gucs_max_prepared_transactions',\n",
    "    'benchmark_config_builtin_script'\n",
    "#     'benchmark_config_time',\n",
    "#     'postgres_gucs_min_wal_size',\n",
    "#     'benchmark_config_transactions'\n",
    "#     'postgres_pgbench_prewarm',\n",
    "]\n",
    "\n",
    "def print_filenames(runs):\n",
    "    filenames = {run.id: run.filename for run in runs}\n",
    "    pdf = pd.Series(filenames)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(pdf)\n",
    "\n",
    "# TODO: rewrite this garbage\n",
    "# Also, previously we wanted disk and instance information always in the title\n",
    "# TODO: add size of memory to title\n",
    "def run_db_size_gb(run, when):\n",
    "    return int(int(run.stats[when]['db_size']) / 1024 / 1024 / 1024)\n",
    "\n",
    "def run_sb_size_mb(run):\n",
    "    sb_size = run.metadata.get('postgres_gucs_shared_buffers', None)\n",
    "    if not sb_size:\n",
    "        return None\n",
    "    return int(int(sb_size) * 8192 / 1024 / 1024)\n",
    "\n",
    "def mem_size_gb(run):\n",
    "    mem_size_bytes = run.metadata.get('machine_instance_mem_total_bytes')\n",
    "    return int(int(mem_size_bytes) / 1024 / 1024 / 1024)\n",
    "\n",
    "\n",
    "def titular_db_size(runs):\n",
    "    min_pre_size = run_db_size_gb(runs[0], 'post_load_pre_run')\n",
    "    max_pre_size = 0\n",
    "    min_post_size = run_db_size_gb(runs[0], 'post_load_post_run')\n",
    "    max_post_size = 0\n",
    "    \n",
    "    sb_size = run_sb_size_mb(runs[0])\n",
    "    \n",
    "    for run in runs:\n",
    "        cur_pre_size = run_db_size_gb(run, 'post_load_pre_run')\n",
    "        cur_post_size = run_db_size_gb(run, 'post_load_post_run')\n",
    "        if cur_pre_size < min_pre_size:\n",
    "            min_pre_size = cur_pre_size\n",
    "        if cur_pre_size > max_pre_size:\n",
    "            max_pre_size = cur_pre_size\n",
    "        if cur_post_size < min_post_size:\n",
    "            min_post_size = cur_post_size\n",
    "        if cur_post_size > max_post_size:\n",
    "            max_post_size = cur_post_size\n",
    "        \n",
    "        if sb_size is None:\n",
    "            continue \n",
    "        run_sb_size = run_sb_size_mb(run)\n",
    "\n",
    "        if run_sb_size is None or sb_size != run_sb_size:\n",
    "            sb_size = None\n",
    "            continue\n",
    "    \n",
    "    mem_size = f'Machine Mem: {mem_size_gb(run)} GB.'\n",
    "    \n",
    "    sb_size_str = ''\n",
    "    if sb_size is not None:\n",
    "        if sb_size > 1024:\n",
    "            sb_size_str = f'SB Size: {int(sb_size / 1024)} GB.'\n",
    "        else:\n",
    "            sb_size_str = f'SB Size: {sb_size} MB.'\n",
    "\n",
    "    if min_pre_size == max_pre_size:\n",
    "        before_str = f\"Before DB Size: {min_pre_size}GB.\"\n",
    "    else:\n",
    "        before_str = f\"Before DB Size: {min_pre_size}-{max_pre_size}GB.\"\n",
    "        \n",
    "    if min_post_size == max_post_size:\n",
    "        after_str = f\"After DB Size: {max_post_size}GB.\"\n",
    "    else:\n",
    "        after_str = f\"After DB Size: {min_post_size}-{max_post_size}GB.\"\n",
    "\n",
    "    return ' '.join([mem_size, sb_size_str, before_str, after_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e9bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "def to_unique_list(store):\n",
    "    result = []\n",
    "    for name, vals in store.items():\n",
    "        for val in vals:\n",
    "            result.append(name + '_' + val)\n",
    "    return result\n",
    "\n",
    "def old_and_short(all_data):\n",
    "    # exclude all data older than 1 day and shorter than 5 seconds\n",
    "    # duration should be in seconds\n",
    "    duration = all_data['data']['pgbench']['summary'].get('duration', None)\n",
    "    if duration and duration < 40:\n",
    "        return True\n",
    "    datetime_str = all_data['metadata']['datetime']\n",
    "    now = datetime.now(timezone.utc)\n",
    "    runtime = datetime.fromisoformat(datetime_str)\n",
    "    if now - runtime > timedelta(days=3, hours=1):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def scale_not_match(all_data):\n",
    "    benchmark_config = all_data['metadata']['benchmark']['config']\n",
    "    scale = benchmark_config.get('scale', None)\n",
    "    if scale is None:\n",
    "        return True\n",
    "#     return scale != 1200 and scale != 3900\n",
    "    return scale != 42\n",
    "\n",
    "def cpufreq_not_null(all_data):\n",
    "    instance = all_data['metadata']['machine']['instance']\n",
    "    cpufreq_gov = instance.get('cpufreq_governor', None)\n",
    "    return cpufreq_gov is None\n",
    "\n",
    "def custom_filename_equals(all_data):\n",
    "    config = all_data['metadata']['benchmark']['config']\n",
    "    custom_filename = config.get('custom_filename', None)\n",
    "    if custom_filename is None:\n",
    "        return True\n",
    "    if custom_filename.startswith('select'):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6fc17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from loader import Loader\n",
    "\n",
    "load_root = 'run_data_local_fresh/backend_flush_after'\n",
    "\n",
    "def pgbench_data_def(all_data):\n",
    "    ys = ['tps', 'ts']\n",
    "    data = all_data['data']['pgbench']['progress']\n",
    "    rows = []\n",
    "    for row in data:\n",
    "        rows.append({k: v for k, v in row.items() if k in ys})\n",
    "    return rows\n",
    "\n",
    "def metadata_def(all_data):\n",
    "    return all_data['metadata']\n",
    "\n",
    "def stats(all_data):\n",
    "    return all_data['stats']\n",
    "\n",
    "loader = Loader(load_root)\n",
    "# loader.discard(old_and_short)\n",
    "# loader.discard(scale_not_match)\n",
    "# loader.discard(cpufreq_not_null)\n",
    "# loader.discard(custom_filename_equals)\n",
    "\n",
    "\n",
    "runs = loader.run({'pgbench': pgbench_data_def}, metadata_def, stats, 'ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9283f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from renderer import SubfigureRenderer\n",
    "from benchart import BenchArt\n",
    "\n",
    "relabels = {\n",
    "    'machine_instance_hostinfo_Hostname': 'host',\n",
    "    'machine_instance_hostinfo_KernelRelease': 'kernel',\n",
    "    'machine_disk_block_device_settings_nr_hw_queues': '#hwq',\n",
    "    'machine_disk_block_device_settings_nr_requests': 'nr_requests',\n",
    "    'machine_disk_block_device_settings_read_ahead_kb': 'read_ahead_kb',\n",
    "    'machine_disk_block_device_settings_queue_depth': 'queue_depth',\n",
    "    'machine_disk_block_device_settings_scheduler': 'scheduler',\n",
    "    'machine_instance_hostinfo_Hostname': 'host',\n",
    "    'benchmark_config_scale': 'pgbench_scale',\n",
    "    'postgres_gucs_backend_flush_after': 'backend_flush_after',\n",
    "    'postgres_gucs_shared_buffers': 'shared_buffers',\n",
    "}\n",
    "\n",
    "benchart = BenchArt(runs)\n",
    "\n",
    "# TODO: make sure this all still works as intended\n",
    "# benchart.part(SubfigureRenderer(relabels), 'benchmark_config_scale')\n",
    "# benchart.part(SubfigureRenderer(relabels), 'postgres_gucs_shared_buffers')\n",
    "# benchart.versus('postgres_gucs_backend_flush_after', 'postgres_gucs_shared_buffers')\n",
    "# benchart.ignore(*ignores, *ignores2)\n",
    "# benchart.versus( 'postgres_gucs_wal_buffers_full')\n",
    "# benchart.versus('postgres_gucs_wal_segment_size')\n",
    "# benchart.versus('benchmark_config_mode')\n",
    "# benchart.versus('postgres_pgbench_prewarm')\n",
    "# benchart.versus('postgres_gucs_shared_buffers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce566af4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from renderer import render, SubfigureRenderer\n",
    "\n",
    "figure = plt.figure(figsize=(15,8), constrained_layout=True)\n",
    "figure.suptitle('pgbench')\n",
    "figure.supylabel('tps', fontsize=16)\n",
    "\n",
    "root, title = render(benchart, figure, timebounds=(0,None), relabels=relabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da152cf3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from loader import Loader\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# TODO: refactor some of the repetitition around checking\n",
    "# if the keys are in the keys list and making the new list of rows\n",
    "def iostat_data_def(all_data):\n",
    "    iostat_ys = [ 'r/s', 'w/s', 'rkB/s', 'wkB/s', 'rrqm/s', 'wrqm/s', 'rrqm','wrqm',\n",
    "               'r_await','w_await', 'rareq-sz','wareq-sz','aqu-sz','util', 'ts', 'user',\n",
    "                'system', 'f/s']\n",
    "    storage_io = []\n",
    "    iostat_data = all_data['data'].get('iostat', None)\n",
    "    for row in iostat_data:\n",
    "        storage_row = row['disk'][0]\n",
    "        storage_row['ts'] = row['timestamp']\n",
    "        cpu_row = row['avg-cpu']\n",
    "        new_cpu_row = {k:v for k, v in cpu_row.items() if k in iostat_ys}\n",
    "        new_storage_row = {k:v for k, v in storage_row.items() if k in iostat_ys}\n",
    "        new_storage_row.update(new_cpu_row)\n",
    "        storage_io.append(new_storage_row)\n",
    "    return storage_io\n",
    "\n",
    "def pgbench_data_def(all_data):\n",
    "    ys = ['tps', 'ts', 'tps_cumulative', 'lat']\n",
    "    data = all_data['data']['pgbench']['progress']\n",
    "    rows = []\n",
    "    tps_sum = 0\n",
    "    for row in data:\n",
    "        tps_sum += int(row['tps'])\n",
    "        row['tps_cumulative'] = tps_sum\n",
    "        rows.append({k: v for k, v in row.items() if k in ys})\n",
    "    return rows\n",
    "\n",
    "def meminfo_data_def(all_data):\n",
    "#     meminfo_ys = ['dirty_kb', 'writeback_kb','ts']\n",
    "    meminfo_ys = ['dirty_kb', 'writeback_kb','mem_free_kb', 'mem_available_kb','ts']\n",
    "\n",
    "    dirtywriteback = all_data['data'].get('dirtywriteback', None)\n",
    "    # dirtywriteback was renamed to meminfo\n",
    "    meminfo = all_data['data'].get('meminfo', None)\n",
    "    if meminfo is None and dirtywriteback is None:\n",
    "        return None\n",
    "    data = meminfo if meminfo is not None else dirtywriteback\n",
    "    rows = []\n",
    "    for row in data:\n",
    "        new_row = {k:v for k, v in row.items() if k in meminfo_ys}\n",
    "        rows.append(new_row)\n",
    "    return rows\n",
    "\n",
    "def metadata_def(all_data):\n",
    "    return all_data['metadata']\n",
    "\n",
    "def stats(all_data):\n",
    "    return all_data['stats']\n",
    "\n",
    "# Note that this only works if run in the same timezone as the pidstat command was run in\n",
    "def pidstat_data_def(all_data):\n",
    "    pidstat_ys = ['kB_wr/s','ts']\n",
    "    pidstat = all_data['data'].get('pidstat', None)\n",
    "    if not pidstat:\n",
    "        return None\n",
    "    if not isinstance(pidstat, list):\n",
    "        return None\n",
    "    tz = datetime.now(timezone.utc).astimezone().tzinfo\n",
    "    procname = pidstat[0]['name']\n",
    "    pidstat = pidstat[0]['data']\n",
    "    rows = []\n",
    "    for row in pidstat:\n",
    "        new_row = {procname + '_' + k:v for k, v in row.items() if k in pidstat_ys and k != 'ts'}\n",
    "        new_row['ts'] = datetime.fromtimestamp(int(row['ts']), tz)\n",
    "        rows.append(new_row)\n",
    "    return rows\n",
    "\n",
    "def walstat_data_def(all_data):\n",
    "    wal_stat_ys = ['ts', 'wal_buffers_full', 'wal_bytes', 'wal_sync', 'wal_fpi',\n",
    "                  'wal_sync_time', 'wal_write_time']\n",
    "    wal_stat_progress = all_data['data'].get('walstat', None)\n",
    "    rows = []\n",
    "    if wal_stat_progress is None:\n",
    "        return None\n",
    "    for row in wal_stat_progress:\n",
    "        for k, v in row.items():\n",
    "            if k == 'ts' or k == 'stats_reset':\n",
    "                continue\n",
    "            row[k] = int(row[k])        \n",
    "        new_row = {k: v for k, v in row.items() if k in wal_stat_ys}\n",
    "        rows.append(new_row)\n",
    "    return rows\n",
    "\n",
    "def iocheckpointerstat_data_def(all_data):\n",
    "    cp_stat_ys = ['ts', 'writes', 'write_time', 'fsyncs', 'fsync_time',\n",
    "                  'writebacks', 'writeback_time']\n",
    "    progress = all_data['data'].get('iocheckpointerstat', None)\n",
    "    if progress is None:\n",
    "        return None\n",
    "    rows = []\n",
    "    for row in progress:\n",
    "        for k, v in row.items():\n",
    "            if k not in cp_stat_ys or k == 'ts':\n",
    "                continue\n",
    "            if k.endswith('time'):\n",
    "                row[k] = float(row[k])\n",
    "            else:\n",
    "                row[k] = int(row[k])\n",
    "        new_row = {k: v for k, v in row.items() if k in cp_stat_ys}\n",
    "        rows.append(new_row)\n",
    "    return rows\n",
    "    \n",
    "def autovac_chr_data_def(all_data):\n",
    "    data = all_data['data'].get('pgiostat', None)\n",
    "    if data is None:\n",
    "        return None\n",
    "    rows = []\n",
    "    for row in data:\n",
    "        if row['backend_type'] != 'autovacuum worker':\n",
    "            continue\n",
    "        if row['object'] != 'relation' or row['context'] != 'vacuum':\n",
    "            continue\n",
    "        hits = int(row['hits'])\n",
    "        reads = int(row['reads'])\n",
    "        if hits + reads == 0:\n",
    "            cache_hit_ratio = 0\n",
    "        else:\n",
    "            cache_hit_ratio = hits / (hits + reads)\n",
    "        new_row = {} \n",
    "        new_row['cache_hit_ratio'] = cache_hit_ratio\n",
    "        new_row['ts'] = row['ts']\n",
    "        rows.append(new_row)\n",
    "    return rows\n",
    "    \n",
    "def backend_chr_data_def(all_data):\n",
    "    data = all_data['data'].get('pgiostat', None)\n",
    "    if data is None:\n",
    "        return None\n",
    "    rows = []\n",
    "    for row in data:\n",
    "        if row['backend_type'] != 'client backend':\n",
    "            continue\n",
    "        if row['object'] != 'relation' or row['context'] != 'normal':\n",
    "            continue\n",
    "        hits = int(row['hits'])\n",
    "        reads = int(row['reads'])\n",
    "        if hits + reads == 0:\n",
    "            cache_hit_ratio = 0\n",
    "        else:\n",
    "            cache_hit_ratio = hits / (hits + reads)\n",
    "        new_row = {} \n",
    "        new_row['cache_hit_ratio'] = cache_hit_ratio\n",
    "        new_row['ts'] = row['ts']\n",
    "        rows.append(new_row)\n",
    "    return rows\n",
    "    \n",
    "def relation_size_data_def(all_data):\n",
    "    data = all_data['data'].get('relationsize', None)\n",
    "    if data is None:\n",
    "        return None\n",
    "    rows = []\n",
    "    for row in data:\n",
    "        new_row = {}\n",
    "        new_row['relation_size'] = int(row['relation_size'])\n",
    "        new_row['ts'] = row['ts']\n",
    "        rows.append(new_row)\n",
    "    return rows\n",
    "    \n",
    "def buffercache_data_def(all_data):\n",
    "    data = all_data['data'].get('buffercache', None)\n",
    "    if data is None:\n",
    "        return None\n",
    "    rows = []\n",
    "    for row in data:\n",
    "        for k, v in row.items():\n",
    "            if k == 'ts':\n",
    "                continue\n",
    "            if k == 'usagecount_avg':\n",
    "                row[k] = float(row[k])\n",
    "            else:\n",
    "                row[k] = int(row[k])\n",
    "        rows.append(row)\n",
    "    if not rows:\n",
    "        return None\n",
    "    return rows\n",
    "    \n",
    "def aggwaits_data_def(all_data):\n",
    "    data = all_data['data'].get('waits', None)\n",
    "    if data is None:\n",
    "        return None\n",
    "    rows = []\n",
    "    for row in data:\n",
    "        col = row['wait_event_type'] + '_' + row['wait_event']\n",
    "        new_row = {}\n",
    "        new_row[col] = int(row['count'])\n",
    "        new_row['ts'] = row['ts']\n",
    "        rows.append(new_row)\n",
    "    return rows\n",
    "    \n",
    "data_exprs = {'pgbench': pgbench_data_def, 'iostat':iostat_data_def,\n",
    "              'meminfo':meminfo_data_def, 'pidstat': pidstat_data_def,\n",
    "             'walstat': walstat_data_def, 'iocheckpointerstat': iocheckpointerstat_data_def,\n",
    "             'buffercache': buffercache_data_def, 'waits': aggwaits_data_def,\n",
    "             'autovaccachehitratio': autovac_chr_data_def,\n",
    "             'backendcachehitratio': backend_chr_data_def,\n",
    "             'relationsize': relation_size_data_def}\n",
    "\n",
    "loader = Loader(load_root)\n",
    "\n",
    "# loader.discard(old_and_short)\n",
    "# loader.discard(scale_not_match)\n",
    "# loader.discard(cpufreq_not_null)\n",
    "# loader.discard(custom_filename_equals)\n",
    "\n",
    "\n",
    "runs = loader.run(data_exprs, metadata_def, stats, 'ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7233924",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from benchart import BenchArt\n",
    "\n",
    "benchart = BenchArt(runs)\n",
    "\n",
    "# benchart.part(None, 'benchmark_config_scale')\n",
    "# benchart.part(None, 'postgres_gucs_shared_buffers')\n",
    "benchart.ignore(*ignores, *ignores2)\n",
    "benchart.versus('postgres_gucs_backend_flush_after')\n",
    "# benchart.versus('postgres_gucs_shared_buffers')\n",
    "# benchart.versus('machine_instance_cpufreq_governor')\n",
    "# benchart.versus('postgres_gucs_huge_pages')\n",
    "# benchart.versus('postgres_gucs_wal_buffers')\n",
    "# benchart.versus('postgres_gucs_wal_segment_size', \n",
    "#                 'postgres_init', 'postgres_pause_after_load')\n",
    "# benchart.versus('postgres_init', 'postgres_pause_after_load')\n",
    "# benchart.versus('machine_disk_block_device_settings_read_ahead_kb', 'machine_disk_dmdelay')\n",
    "# benchart.versus('machine_disk_block_device_settings_read_ahead_kb')\n",
    "\n",
    "# benchart.versus('postgres_gucs_wal_compression')\n",
    "# benchart.versus('postgres_gucs_autovacuum_vacuum_cost_delay')\n",
    "# benchart.versus('benchmark_config_mode')\n",
    "# benchart.versus('postgres_pgbench_prewarm')\n",
    "# benchart.versus('benchmark_config_custom_filename', 'postgres_gucs_shared_buffers')\n",
    "# benchart.versus('benchmark_data_distribution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f3b37b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from renderer import render_multi, render_print_tree\n",
    "\n",
    "skip_keys = ['postgres_gucs_unix_socket_permissions', 'postgres_gucs_server_encoding',\n",
    "                 'postgres_gucs_application_name', 'postgres_gucs_TimeZone', \n",
    "             'postgres_gucs_data_directory_mode',\n",
    "             'postgres_gucs_archive_command', 'postgres_gucs_client_encoding', \n",
    "             'postgres_gucs_log_file_mode',\n",
    "             'postgres_gucs_log_timezone', 'postgres_gucs_default_text_search_config', \n",
    "            ]\n",
    "\n",
    "# root = benchart.run()\n",
    "# render_print_tree(root, occludes=benchart.ignores)\n",
    "\n",
    "sorted_prefixes = ['pgbench', 'iostat', 'meminfo', 'pidstat', \n",
    "                   'walstat', 'iocheckpointerstat',\n",
    "                  'buffercache', 'waits', 'autovaccachehitratio',\n",
    "                  'backendcachehitratio', 'relationsize']\n",
    "\n",
    "def axes_expr(axes):\n",
    "    axes['iostat_wrqm'].set_yscale('log')\n",
    "#     axes['iostat_rrqm'].set_yscale('log')\n",
    "#     axes['iostat_rrqm/s'].set_yscale('log')\n",
    "    axes['iostat_wrqm'].set_yscale('log')\n",
    "    axes['iostat_wrqm/s'].set_yscale('log')\n",
    "    axes['iostat_r_await'].set_yscale('log')\n",
    "    axes['iostat_w_await'].set_yscale('log')\n",
    "    \n",
    "def fake_axes_expr(axes):\n",
    "    pass\n",
    "\n",
    "figwidth = 15\n",
    "timebounds=(0,None)\n",
    "root, title = render_multi(benchart, figwidth, sorted_prefixes, timebounds, relabels,\n",
    "                           titular_db_size, fake_axes_expr)\n",
    "\n",
    "for k, v in root.metadata.items():\n",
    "    gpl = k.split('_')\n",
    "\n",
    "    if gpl[0] != 'postgres' or gpl[1] != 'gucs':\n",
    "        continue\n",
    "    if gpl[2] == 'lc':\n",
    "        continue\n",
    "    if k in skip_keys:\n",
    "        continue\n",
    "    print(f'{k}:{v}\\n')\n",
    "\n",
    "# plt.savefig('example.png', transparent=False, facecolor='white', edgecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4904692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "879044bb",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
